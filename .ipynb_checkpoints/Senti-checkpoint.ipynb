{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "from urllib.request import urlopen\n",
    "from textblob import TextBlob\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_text(url):\n",
    "#     Parameters\n",
    "#     url : string : contains url for website\n",
    "#     Return\n",
    "#     text : string: contains text from url\n",
    "    request = requests.get(url).text\n",
    "    soup = BeautifulSoup(request, \"html.parser\")\n",
    "    p_tags = soup.find_all('p')\n",
    "    p_tags_text = [tag.get_text().strip() for tag in p_tags]\n",
    "    text = ' '.join(p_tags_text)\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_string='https://news.google.com/search?q='\n",
    "end_string='&hl=en-US&gl=US&ceid=US%3Aen'\n",
    "def search_url(query):\n",
    "#     Parameters\n",
    "#     query_url : string : query\n",
    "#     Return\n",
    "#     query : string: contains url for search\n",
    "    query=query.split()\n",
    "    query='%20'.join(query)\n",
    "    query_url=start_string+query+end_string\n",
    "    return query_url"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def full_results(query_url):\n",
    "#     Parameters\n",
    "#     url : string : url of query\n",
    "#     Return\n",
    "#     results : list: contains list of dictionaries\n",
    "#     each dictionary has the keys 'url', 'title', 'image', 'time', 'publisher' with the corresponding values for that ith search result\n",
    "#     (i being the index in the list)\n",
    "#     this function is meant to find all of the necessary info for cards from queries\n",
    "    num=10\n",
    "    request = requests.get(query_url).text\n",
    "    soup = BeautifulSoup(request, \"html.parser\")\n",
    "    result_tags = soup.find_all('div', {'jslog': '93789'})\n",
    "    results=[]\n",
    "    special_result_tags = soup.find_all('div', {'jscontroller': 'd0DtYd'})\n",
    "    for i in range(len(special_result_tags)):\n",
    "        results.append({})\n",
    "        for element in special_result_tags[i].contents[0].contents[0].contents[0].contents[0].contents:\n",
    "                if element['class']==['tvs3Id', 'QwxBBf']:\n",
    "                    results[i]['image']=element['src']\n",
    "        results[i]['url']='https://news.google.com/'+special_result_tags[i].contents[0].contents[0].contents[1].contents[0]['href'].strip('./')\n",
    "        results[i]['title']=special_result_tags[i].contents[0].contents[0].contents[1].contents[1].contents[0].contents[0]\n",
    "        results[i]['time']=special_result_tags[i].contents[0].contents[0].contents[1].contents[3].contents[0].contents[3].contents[0]\n",
    "        results[i]['publisher']=special_result_tags[i].contents[0].contents[0].contents[1].contents[3].contents[0].contents[2].contents[0]\n",
    "    if len(special_result_tags)>num-1:\n",
    "        return results[:num]\n",
    "    for i in range(min(len(result_tags),num-len(special_result_tags))):\n",
    "        j=i+len(special_result_tags)\n",
    "        results.append({})\n",
    "        if len(result_tags[i]['class'])==6:\n",
    "            for element in result_tags[i].contents[0].contents[0].contents:\n",
    "                if element['class']==['tvs3Id', 'QwxBBf']:\n",
    "                    results[j]['image']=element['src']\n",
    "            results[j]['url']='https://news.google.com/'+result_tags[i].contents[1].contents[0].contents[0]['href'].strip('./')\n",
    "            results[j]['title']=result_tags[i].contents[1].contents[0].contents[1].contents[0].contents[0]\n",
    "            results[j]['time']=result_tags[i].contents[1].contents[0].contents[3].contents[0].contents[3].contents[0]\n",
    "            results[j]['publisher']=result_tags[i].contents[1].contents[0].contents[3].contents[0].contents[2].contents[0]\n",
    "        if len(result_tags[i]['class'])==5:\n",
    "            results[j]['image']=False\n",
    "            results[j]['url']='https://news.google.com/'+result_tags[i].contents[0].contents[0].contents[0]['href'].strip('./')\n",
    "            results[j]['title']=result_tags[i].contents[0].contents[0].contents[1].contents[0].contents[0]\n",
    "            results[j]['time']=result_tags[i].contents[0].contents[0].contents[3].contents[0].contents[3].contents[0]\n",
    "            results[j]['publisher']=result_tags[i].contents[0].contents[0].contents[3].contents[0].contents[2].contents[0]\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def only_urls(query_url,num):\n",
    "#     Parameters\n",
    "#     url : string : url of query\n",
    "#     num : int : number of article urls to harvest\n",
    "#     Return\n",
    "#     results : list: contains list of dictionaries\n",
    "#     each dictionary has the key 'url' with the corresponding value for that ith search result (i being the index in the list)\n",
    "#     this function is meant to find only the url of the article\n",
    "    request = requests.get(query_url).text\n",
    "    soup = BeautifulSoup(request, \"html.parser\")\n",
    "    result_tags = soup.find_all('div', {'jslog': '93789'})\n",
    "    results=[]\n",
    "    special_result_tags = soup.find_all('div', {'jscontroller': 'd0DtYd'})\n",
    "    for i in range(len(special_result_tags)):\n",
    "        results.append({})\n",
    "        results[i]['url']='https://news.google.com/'+special_result_tags[i].contents[0].contents[0].contents[1].contents[0]['href'].strip('./')\n",
    "    if len(special_result_tags)>num-1:\n",
    "        return results[:num]\n",
    "    for i in range(min(len(result_tags),num-len(special_result_tags))):\n",
    "        j=i+len(special_result_tags)\n",
    "        results.append({})\n",
    "        if len(result_tags[i]['class'])==6:\n",
    "            results[j]['url']='https://news.google.com/'+result_tags[i].contents[1].contents[0].contents[0]['href'].strip('./')\n",
    "        if len(result_tags[i]['class'])==5:\n",
    "            results[j]['url']='https://news.google.com/'+result_tags[i].contents[0].contents[0].contents[0]['href'].strip('./')\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_sentiments(results, sort):\n",
    "#     Parameters\n",
    "#     results : list: contains list of dictionaries\n",
    "#     sort : bool: whether or not to sort by polarity\n",
    "#     each dictionary has the keys 'url', 'title', 'image' with the corresponding values for that ith search result\n",
    "#     (i being the index in the list)\n",
    "#     Return\n",
    "#     results : list: contains list of dictionaries\n",
    "#     each dictionary has the keys 'url', 'title', 'image', 'polarity', 'subjectivity' with the corresponding values for that\n",
    "#     ith search result (i being the index in the list)\n",
    "    for i in range(len(results)):\n",
    "        if 'url' in results[i].keys():\n",
    "            tb=TextBlob(extract_text(results[i]['url']))\n",
    "            results[i]['polarity']=np.round(((tb.polarity)+1)/2,4)\n",
    "            results[i]['subjectivity']=np.round(tb.subjectivity,4)\n",
    "    if sort:\n",
    "        results=sorted(results, key=lambda result: result['polarity'])\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "countries=[\n",
    "    'United States',\n",
    "    'India',\n",
    "    'South Africa',\n",
    "    'United Kingdom',\n",
    "    'Singapore',\n",
    "    'Canada',\n",
    "    'Australia',\n",
    "    'Nigeria'\n",
    "\n",
    "]\n",
    "country_urls=[\n",
    "    'https://news.google.com/topstories?hl=en-US&gl=US&ceid=US%3Aen',\n",
    "    'https://news.google.com/topstories?hl=en-IN&gl=IN&ceid=IN:en',\n",
    "    'https://news.google.com/topstories?hl=en-ZA&gl=ZA&ceid=ZA:en',\n",
    "    'https://news.google.com/topstories?hl=en-GB&gl=GB&ceid=GB:en',\n",
    "    'https://news.google.com/topstories?hl=en-SG&gl=SG&ceid=SG:en',\n",
    "    'https://news.google.com/topstories?hl=en-CA&gl=CA&ceid=CA:en',\n",
    "    'https://news.google.com/topstories?hl=en-AU&gl=AU&ceid=AU:en',\n",
    "    'https://news.google.com/topstories?hl=en-NG&gl=NG&ceid=NG:en'\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
